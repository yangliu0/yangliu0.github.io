<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2017%2F12%2F13%2Ftest%2F</url>
    <content type="text"><![CDATA[L(w,b)=−∑xiεnyi(wxi+b)L\left( w,b\right) =-\sum _{x_{i\varepsilon }n}y_i\left(wx_i+b\right)L(w,b)=−∑​x​iε​​n​​y​i​​(wx​i​​+b)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Perceptron(感知机)算法介绍及实现]]></title>
    <url>%2F2017%2F12%2F13%2FPerceptron-%E6%84%9F%E7%9F%A5%E6%9C%BA-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[感知机： 假设输入空间是χ⊆Rn\chi\subseteq R^nχ⊆R​n​​,输出空间是γ=(+1,−1)\gamma =\left( +1,-1\right)γ=(+1,−1)。输入χ∈X\chi\in Xχ∈X表示实例的特征向量，对应于输入空间的点；输出y∈γy\in \gammay∈γ表示实例的类别。由输入空间到输出空间的如下函数： f(x)=sign(wx+b)f\left( x\right) =sign\left( wx+b\right) f(x)=sign(wx+b) 称为感知机。其中，w和b为感知机模型的参数，sign是符号函数，即： sign(x)={+1x≥0−1x&lt;0sign\left( x\right) =\begin{cases}+1 &amp;x\geq 0\\-1 &amp;x&lt;0 \end{cases} sign(x)={​+1​−1​​​x≥0​x&lt;0​​ 感知机学习策略 假设训练数据集是线性可分的，感知机学习的目标就是求得一个能够将训练集正实例点和负实例点完成正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数w，b，需定义损失函数并将损失函数极小化。 损失函数使用误分类点到超平面S的总距离，因此输入空间R^n​中任何一点x_0​到超平面S的距离为： 1∥w∥∣wx+b∣\dfrac {1}{\left\| w\right\| }\left| wx+b\right| ​∥w∥​​1​​∣wx+b∣ 在这里，∥w∥\left\| w\right\|∥w∥是w的L2L_2L​2​​范数。 对于误分类的数据(xi,yi)\left(x_i,y_i\right)(x​i​​,y​i​​)来说， −yi(wxi+b)&gt;0-y_i( wx_i+b) &gt;0 −y​i​​(wx​i​​+b)&gt;0 成立。因为当wxi+b&gt;0wx_i + b &gt;0wx​i​​+b&gt;0时，yi=−1y_i=-1y​i​​=−1，而当wxi+b&lt;0wx_i + b &lt; 0wx​i​​+b&lt;0时，yi=+1y_i=+1y​i​​=+1，因此，误分类点xix_ix​i​​到超平面S的距离是: −1∥w∥yi(wxi+b)-\dfrac {1}{\left\| w\right\| }y_i( wx_i+b) −​∥w∥​​1​​y​i​​(wx​i​​+b) 这样，假设超平面S的误分类点集合为M，那么所以误分类点到超平面S的总距离为： −1∥w∥∑xi∈Myi(wxi+b)-\dfrac {1}{\left\| w\right\| }\sum _{x_{i} \in M}y_{i}\left( wx_{i}+b\right) −​∥w∥​​1​​​x​i​​∈M​∑​​y​i​​(wx​i​​+b) 不考虑1∥w∥\dfrac {1}{\left\| w\right\| }​∥w∥​​1​​，就得到感知机学习的损失函数： L(w,b)=−∑xiεnyi(wxi+b)L\left( w,b\right) =-\sum _{x_{i\varepsilon }n}y_i\left(wx_i+b\right) L(w,b)=−​x​iε​​n​∑​​y​i​​(wx​i​​+b) 其中M为误分类点的集合。 感知机算法原始形式 感知机算法是使得损失函数L(w,b)L(w,b)L(w,b)极小化的最优化问题，可以使用随机梯度下降法来进行最优化。 假设误分类点集合M是固定的，那么损失函数L(w,b)L(w,b)L(w,b)的梯度由 ∇wL(ω,b)=−∑xiεnyixi\nabla _{w}L\left( \omega ,b\right) =-\sum _{x_{i\varepsilon }n}y_{i}x_{i} ∇​w​​L(ω,b)=−​x​iε​​n​∑​​y​i​​x​i​​ ∇bL(ω,b)=−∑xiεnyi\nabla _{b}L\left( \omega ,b\right) =-\sum _{x_{i\varepsilon }n}y_{i} ∇​b​​L(ω,b)=−​x​iε​​n​∑​​y​i​​ 给出，随机选取一个误分类点(xi,yi)(x_i,y_i)(x​i​​,y​i​​)，对w，b进行更新： w←w+ηyixiw\leftarrow w+\eta y_ix_i w←w+ηy​i​​x​i​​ b←b+ηyib\leftarrow b+\eta y_i b←b+ηy​i​​ 其中η(0&lt;η≤1)\eta(0&lt;\eta \leq1)η(0&lt;η≤1) 称为学习率(learning rate)，这样通过迭代可以使得损失函数L(w,b)L(w,b)L(w,b)不断减小，直到为0。 感知机算法原始形式的主要训练过程： 1234567891011121314151617181920def trainPerceptron(dataMat, labelMat, eta): m, n = dataMat.shape weight = np.zeros(n) bias = 0 flag = True while flag: for i in range(m): if np.any(labelMat[i] * (np.dot(weight, dataMat[i]) + bias) &lt;= 0): weight = weight + eta * labelMat[i] * dataMat[i].T bias = bias + eta * labelMat[i] print("weight, bias: ", end="") print(weight, end=" ") print(bias) flag = True break else: flag = False return weight, bias 完整代码可以前往我的github查看，还有可视化过程。 https://github.com/yangliu0/MachineLearning/tree/master/Perceptron 感知机算法的对偶形式 假设w_{0}，b_{0}均初始化为0，对误分类点通过 w←w+ηyixiw\leftarrow w+\eta y_ix_i w←w+ηy​i​​x​i​​ b←b+ηyib\leftarrow b+\eta y_i b←b+ηy​i​​ 逐步修改w，b，设修改n次，则w，b关于(wi,yi)(w_i,y_i)(w​i​​,y​i​​)的增量分为是αiyixi\alpha_i y_ix_iα​i​​y​i​​x​i​​和αiyi\alpha_i y_iα​i​​y​i​​，这里的αi=niη\alpha_i=n_i \etaα​i​​=n​i​​η，其中nin_in​i​​表示第i个点误分类的次数，这样最后学习到的w,bw,bw,b可以分别表示为 w=∑i=1Nαiyixiw=\sum ^{N}_{i=1}\alpha _{i}y_{i}x_{i} w=​i=1​∑​N​​α​i​​y​i​​x​i​​ b=∑i=1Nαiyib=\sum ^{N}_{i=1}\alpha _{i}y_{i} b=​i=1​∑​N​​α​i​​y​i​​ 实例点更新的次数越多，意味着它距离分离超平面越近，也就越难正确分类。 训练过程：输出α,b\alpha,bα,b，其中α=(α1,α2,...,αN)T\alpha = (\alpha_1, \alpha_2,...,\alpha_N)^Tα=(α​1​​,α​2​​,...,α​N​​)​T​​ (1)α←0,b←0\alpha\leftarrow0, b\leftarrow0α←0,b←0 (2)在训练集中选取数据(xi,yi)(x_i, y_i)(x​i​​,y​i​​) (3)如果yi(∑j=1nαjyjxj⋅xi+b)≤0y_{i}\left( \sum ^{n}_{j=1}\alpha _{j}y_{j}x_{j}\cdot x_{i}+b\right)\leq0y​i​​(∑​j=1​n​​α​j​​y​j​​x​j​​⋅x​i​​+b)≤0，则 αi←αi+η\alpha_i \leftarrow\alpha_i + \eta α​i​​←α​i​​+η b←b+ηyib\leftarrow b+\eta y_i b←b+ηy​i​​ (4)转到(2)，直到没有错误。 最后通过w=∑i=1Nαiyixiw=\sum ^{N}_{i=1}\alpha _{i}y_{i}x_{i}w=∑​i=1​N​​α​i​​y​i​​x​i​​计算出www，使用上述过程求出的bbb，即计算出模型参数。 以下就是上述感知机对偶形式的python训练代码： 12345678910111213def trainModel(dataMat, labelMat, alpha, b, eta): flag = True while flag: for i in range(m): if (labelMat[i, 0] * (np.sum((alpha * labelMat * np.dot(dataMat, dataMat[i].T).reshape((m, 1)))) + b)) &lt;= 0: alpha[i] = alpha[i] + eta b = b + eta * labelMat[i] flag = True break else: flag = False w = np.dot(dataMat.T, alpha * labelMat) return w, b 以下就是结果可视化图，可以看出，训练的感知机算法对线性可分的点进行了很好的划分。 感知机算法的原始形式和对偶形式分别对应了支持向量机(SVM)的两种相应形式，是它们的基础。 上述两种形式的感知机算法完整实现可以在我的github查看，使用python实现。 https://github.com/yangliu0/MachineLearning/tree/master/Perceptron 参考文献 [1]统计学习方法.李航]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>MachineLearning</tag>
        <tag>Perceptron</tag>
      </tags>
  </entry>
</search>
